{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w2v.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_DRpv9VAVH-y","colab_type":"code","outputId":"29e5f5b9-0ba7-4a05-9461-23bfa97435d1","executionInfo":{"status":"ok","timestamp":1575797709345,"user_tz":-480,"elapsed":1308,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Upload data manually\n","#from google.colab import  files\n","#uploaded=files.upload()\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AWx28bPJ5-Yv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"76385c7a-9670-454a-c6e7-a3bba13586d7","executionInfo":{"status":"ok","timestamp":1575795587531,"user_tz":-480,"elapsed":4798,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}}},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sun Dec  8 08:59:45 2019       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cb1dzxA5X6bF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fb80d0bf-29e9-408c-d8a0-e63a73c88a88","executionInfo":{"status":"ok","timestamp":1575795659021,"user_tz":-480,"elapsed":14439,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}}},"source":["# Since I'm using colabs, we need actually implement all these into tensorflow modules so that we can use the gpu.\n","%tensorflow_version 1.x\n","file = open('/content/drive/My Drive/Colab Notebooks/Tang_poems_utf_8.txt')\n","text = file.read()\n","file.close()\n","import matplotlib.pyplot as plt\n","## split and create regular poems\n","import re\n","import numpy as np\n","import matplotlib.font_manager as fm\n","import os\n","import csv\n","import time\n","import pandas as pd\n","import seaborn as sns\n","from collections import defaultdict\n","import numpy as np\n","import tensorflow as tf\n","\n","# I'm using WSL2, thus in order to provide chinese support, I need to configure fonts myself\n","plt.rcParams['font.sans-serif']=['Taipei Sans TC Beta']\n","pattern = u'卷.[0-9]+'\n","poems = re.split(pattern, text)[1:]\n","regular_poems = []\n","regular_title = []\n","corpus = []\n","N = 1000 # total number of samples\n","stopwords = re.compile(u'而|何|乎|乃|其|且|然|若|所|为|焉|也|以|矣|于|之|则|者|与|欤|因|-')\n","for poem in poems:\n","    tmp_poem = poem.strip('\\n\\u3000\\u3000◎')\n","    tmp_poem = tmp_poem.replace('\\u3000\\u3000','').split('\\n')\n","    regular_title.append(tmp_poem[0])\n","    regular_poems.append('\\n'.join(tmp_poem[1:]))\n","    tmp_poem=''.join(tmp_poem[1:]).replace('\\n',\"\").replace('。','').replace('，','').replace('：','').replace('；','').replace('？','').replace('！','').replace('（[.*]*?）','')\n","    corpus.append(list(stopwords.sub('',tmp_poem)))\n","    ## Word frequency\n","    \n","#corpus=corpus\n","# corpus=corpus\n","corpus = corpus[0:N]\n","# develop based on this part\n","# create a vocabulary\n","words = []\n","for poem in corpus:\n","    for word in poem:\n","        words.append(word)\n","words = set(words)\n","# create map between word and integers\n","word2int = {}\n","int2word = {}\n","vocab_size = len(words)  # gives the total number of unique words\n","for i, word in enumerate(words):\n","    word2int[word] = i\n","    int2word[i] = word\n","sentences = corpus\n","# capture each word and their neighborhood\n","data = []\n","WINDOW_SIZE = 2\n","n_count = 0\n","for sentence in sentences:\n","    for word_index, word in enumerate(sentence):\n","        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0):min(word_index + WINDOW_SIZE, len(sentence)+1)]:\n","            if nb_word != word:\n","                data.append([word, nb_word])\n","# BUT actually we don't need this, because we consider each character in the poem as a word, that is to say, a vector\n","# convert to unit vector.\n","\n","\n","def unit_vectorization(data_point_index, vocab_size):\n","    temp = np.zeros(vocab_size)\n","    temp[data_point_index] = 1\n","    return temp\n","\n","vocab_size = len(words)\n","x_train = []\n","y_train = []\n","for data_word in data:\n","    x_train.append(unit_vectorization(word2int[data_word[0]], vocab_size))\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","for data_word in data:\n","    y_train.append(unit_vectorization(word2int[data_word[1]], vocab_size))\n","y_train = np.asarray(y_train)\n","\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","y_train = np.asarray(y_train)\n","# make the tensorflow model\n","x = tf.placeholder(tf.float32, [None, vocab_size],name=\"x\")\n","y_label = tf.placeholder(tf.float32, shape=(None, vocab_size),name=\"y_label\")\n","\n","EMBEDDING_DIM = 5\n","W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]),name=\"W1\")\n","# noise ### this line claims a variable of tf type which is essentially a 1x5 matrix\n","b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]),name=\"b1\")\n","hidden_representation = tf.add(tf.matmul(x, W1), b1)\n","W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]),name=\"W2\")\n","b2 = tf.Variable(tf.random_normal([vocab_size]),name=\"b2\")\n","prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation, W2), b2))\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)  # make sure you do this!\n","# define the loss function:\n","print('Preparation Done')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Preparation Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QLNPkliehMnS","colab_type":"code","outputId":"6b851896-5fb4-46c3-98ba-2af392206718","executionInfo":{"status":"ok","timestamp":1575796342822,"user_tz":-480,"elapsed":673352,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["# define the loss function:\n","cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(\n","    y_label * tf.log(prediction), reduction_indices=[1]))\n","# define the training step:\n","train_step = tf.train.GradientDescentOptimizer(\n","    0.1).minimize(cross_entropy_loss)\n","n_iters = 301 # Even on google colab implementing 10000 iterations takes a very looooong time\n","print('Start calculating')\n","# train for n_iter iterations\n","saver = tf.train.Saver()\n","checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/cp.ckpt\" \n","count = 0\n","for _ in range(n_iters):\n","    count += 1\n","    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n","    if count % 10 == 1:\n","      print(str(count)+'th iteration, loss is : ', sess.run(cross_entropy_loss,\n","                                 feed_dict={x: x_train, y_label: y_train}))\n","    if count % 100 == 0:\n","      save_path = saver.save(sess,checkpoint_path,global_step=count)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Start calculating\n","1th iteration, loss is :  13.066922\n","11th iteration, loss is :  11.273584\n","21th iteration, loss is :  10.959448\n","31th iteration, loss is :  10.898855\n","41th iteration, loss is :  10.877644\n","51th iteration, loss is :  10.8628435\n","61th iteration, loss is :  10.849344\n","71th iteration, loss is :  10.836319\n","81th iteration, loss is :  10.823623\n","91th iteration, loss is :  10.811212\n","101th iteration, loss is :  10.799067\n","111th iteration, loss is :  10.7871685\n","121th iteration, loss is :  10.775506\n","131th iteration, loss is :  10.764063\n","141th iteration, loss is :  10.752828\n","151th iteration, loss is :  10.741789\n","161th iteration, loss is :  10.730938\n","171th iteration, loss is :  10.720263\n","181th iteration, loss is :  10.709757\n","191th iteration, loss is :  10.699411\n","201th iteration, loss is :  10.6892185\n","211th iteration, loss is :  10.679173\n","221th iteration, loss is :  10.669268\n","231th iteration, loss is :  10.659494\n","241th iteration, loss is :  10.649852\n","251th iteration, loss is :  10.640331\n","261th iteration, loss is :  10.63093\n","271th iteration, loss is :  10.621643\n","281th iteration, loss is :  10.612466\n","291th iteration, loss is :  10.603395\n","301th iteration, loss is :  10.594427\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wfb9q2I-hbJX","colab_type":"code","outputId":"54a289cb-6159-476b-9655-e60c89fef5d4","executionInfo":{"status":"ok","timestamp":1575796343532,"user_tz":-480,"elapsed":720,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# output\n","vectors = sess.run(W1 + b1)\n","\n","\n","def word_vec(word):\n","    v_w = vectors[word2int[word]]\n","    return v_w\n","\n","\n","def word_sim(word, top_n):\n","    w1_index = word2int[word]\n","    v_w1 = vectors[w1_index]\n","    word_sim = {}\n","    for i in range(vectors.shape[0]):\n","        v_w2 = vectors[i]\n","        theta_num = np.dot(v_w1, v_w2)\n","        theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n","        theta = theta_num / theta_den\n","        word = int2word[i]\n","        word_sim[word] = theta\n","    words_sorted = sorted(word_sim.items(), key=lambda x: x[1], reverse=True)\n","    return words_sorted[1:top_n + 1]\n","\n","\n","\n","filename_w2v = '/content/drive/My Drive/Colab Notebooks/w2v_matrix.txt'\n","with open(filename_w2v, 'w') as f:\n","    f.write(\"字\\t对应向量\\t训练样本数\"+str(N))\n","    for x in words:\n","        f.write(\"\\n\"+x+\"\\t\"+str(word_vec(x)))\n","filename_w2v_sample = \"/content/drive/My Drive/Colab Notebooks/w2v_sample.txt\"\n","sample = ['思', '悲', '忧', '愁', '怒', '惧', '乐']\n","with open(filename_w2v_sample, 'w') as f:\n","    for mu in sample:\n","        f.write(mu+\": \" + \"、\".join([y[0] for y in word_sim(mu, 7)])+\"\\n\")\n","\n","print('Files are saved')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Files are saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jl5l3Vne3AJA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AK0oRqV-2xpq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ge2f_SLA2jch","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQ8uxinwpvC9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":470},"outputId":"f7fc0f61-9bc1-4989-c251-65d8507ab7cb","executionInfo":{"status":"error","timestamp":1575796343534,"user_tz":-480,"elapsed":16,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"}}},"source":["print(\"Optimize again\")\n","count=301\n","for _ in range(n_iters):\n","    count += 1\n","    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n","    if count % 10 == 1:\n","      print(str(count)+'th iteration, loss is : ', sess.run(cross_entropy_loss,\n","                                 feed_dict={x: x_train, y_label: y_train}))\n","    if count % 100 == 0:\n","      save_path = saver.save(sess,checkpoint_path,global_step=count)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Optimize again\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1120\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1121\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3606\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3607\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3680\u001b[0m                     \"\\\"<op_name>:<output_index>\\\".\")\n\u001b[0;32m-> 3681\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The name '貂' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-80d7580e3dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       print(str(count)+'th iteration, loss is : ', sess.run(cross_entropy_loss,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1121\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: ' +\n\u001b[0;32m-> 1123\u001b[0;31m                             e.args[0])\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: The name '貂' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\"."]}]},{"cell_type":"code","metadata":{"id":"O7JIhDfNBUS-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}