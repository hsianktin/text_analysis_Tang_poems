{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w2v.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":14439,"status":"ok","timestamp":1575795659021,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"},"user_tz":-480},"id":"cb1dzxA5X6bF","outputId":"fb80d0bf-29e9-408c-d8a0-e63a73c88a88"},"outputs":[{"name":"stdout","output_type":"stream","text":["Preparation Done\n"]}],"source":["# Since I'm using colabs, we need actually implement all these into tensorflow modules so that we can use the gpu.\n","# %tensorflow_version 1.x\n","file = open('Tang_poems_utf_8.txt')\n","text = file.read()\n","file.close()\n","import matplotlib.pyplot as plt\n","## split and create regular poems\n","import re\n","import numpy as np\n","import os\n","from collections import defaultdict\n","import numpy as np\n","import tensorflow as tf\n","\n","pattern = u'卷.[0-9]+'\n","poems = re.split(pattern, text)[1:]\n","regular_poems = []\n","regular_title = []\n","corpus = []\n","N = 1000 # total number of samples\n","stopwords = re.compile(u'而|何|乎|乃|其|且|然|若|所|为|焉|也|以|矣|于|之|则|者|与|欤|因|-')\n","for poem in poems:\n","    tmp_poem = poem.strip('\\n\\u3000\\u3000◎')\n","    tmp_poem = tmp_poem.replace('\\u3000\\u3000','').split('\\n')\n","    regular_title.append(tmp_poem[0])\n","    regular_poems.append('\\n'.join(tmp_poem[1:]))\n","    tmp_poem=''.join(tmp_poem[1:]).replace('\\n',\"\").replace('。','').replace('，','').replace('：','').replace('；','').replace('？','').replace('！','').replace('（[.*]*?）','')\n","    corpus.append(list(stopwords.sub('',tmp_poem)))\n","    ## Word frequency\n","    \n","#corpus=corpus\n","# corpus=corpus\n","corpus = corpus[:N]\n","# develop based on this part\n","# create a vocabulary\n","words = []\n","for poem in corpus:\n","    for word in poem:\n","        words.append(word)\n","words = set(words)\n","# create map between word and integers\n","word2int = {}\n","int2word = {}\n","vocab_size = len(words)  # gives the total number of unique words\n","for i, word in enumerate(words):\n","    word2int[word] = i\n","    int2word[i] = word\n","sentences = corpus\n","# capture each word and their neighborhood\n","data = []\n","WINDOW_SIZE = 2\n","n_count = 0\n","for sentence in sentences:\n","    for word_index, word in enumerate(sentence):\n","        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0):min(word_index + WINDOW_SIZE, len(sentence)+1)]:\n","            if nb_word != word:\n","                data.append([word, nb_word])\n","# BUT actually we don't need this, because we consider each character in the poem as a word, that is to say, a vector\n","# convert to unit vector.\n","\n","\n","def unit_vectorization(data_point_index, vocab_size):\n","    temp = np.zeros(vocab_size)\n","    temp[data_point_index] = 1\n","    return temp\n","\n","vocab_size = len(words)\n","x_train = []\n","y_train = []\n","for data_word in data:\n","    x_train.append(unit_vectorization(word2int[data_word[0]], vocab_size))\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","for data_word in data:\n","    y_train.append(unit_vectorization(word2int[data_word[1]], vocab_size))\n","y_train = np.asarray(y_train)\n","\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","y_train = np.asarray(y_train)\n","# make the tensorflow model\n","x = tf.placeholder(tf.float32, [None, vocab_size],name=\"x\")\n","y_label = tf.placeholder(tf.float32, shape=(None, vocab_size),name=\"y_label\")\n","\n","EMBEDDING_DIM = 5\n","W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]),name=\"W1\")\n","# noise ### this line claims a variable of tf type which is essentially a 1x5 matrix\n","b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]),name=\"b1\")\n","hidden_representation = tf.add(tf.matmul(x, W1), b1)\n","W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]),name=\"W2\")\n","b2 = tf.Variable(tf.random_normal([vocab_size]),name=\"b2\")\n","prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation, W2), b2))\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)  # make sure you do this!\n","# define the loss function:\n","print('Preparation Done')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"colab_type":"code","executionInfo":{"elapsed":673352,"status":"ok","timestamp":1575796342822,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"},"user_tz":-480},"id":"QLNPkliehMnS","outputId":"6b851896-5fb4-46c3-98ba-2af392206718"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start calculating\n","1th iteration, loss is :  13.066922\n","11th iteration, loss is :  11.273584\n","21th iteration, loss is :  10.959448\n","31th iteration, loss is :  10.898855\n","41th iteration, loss is :  10.877644\n","51th iteration, loss is :  10.8628435\n","61th iteration, loss is :  10.849344\n","71th iteration, loss is :  10.836319\n","81th iteration, loss is :  10.823623\n","91th iteration, loss is :  10.811212\n","101th iteration, loss is :  10.799067\n","111th iteration, loss is :  10.7871685\n","121th iteration, loss is :  10.775506\n","131th iteration, loss is :  10.764063\n","141th iteration, loss is :  10.752828\n","151th iteration, loss is :  10.741789\n","161th iteration, loss is :  10.730938\n","171th iteration, loss is :  10.720263\n","181th iteration, loss is :  10.709757\n","191th iteration, loss is :  10.699411\n","201th iteration, loss is :  10.6892185\n","211th iteration, loss is :  10.679173\n","221th iteration, loss is :  10.669268\n","231th iteration, loss is :  10.659494\n","241th iteration, loss is :  10.649852\n","251th iteration, loss is :  10.640331\n","261th iteration, loss is :  10.63093\n","271th iteration, loss is :  10.621643\n","281th iteration, loss is :  10.612466\n","291th iteration, loss is :  10.603395\n","301th iteration, loss is :  10.594427\n"]}],"source":["# define the loss function:\n","cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(\n","    y_label * tf.log(prediction), reduction_indices=[1]))\n","# define the training step:\n","train_step = tf.train.GradientDescentOptimizer(\n","    0.1).minimize(cross_entropy_loss)\n","n_iters = 301 # Even on google colab implementing 10000 iterations takes a very looooong time\n","print('Start calculating')\n","# train for n_iter iterations\n","saver = tf.train.Saver()\n","checkpoint_path = \"./ckpt/w2v\" \n","count = 0\n","for _ in range(n_iters):\n","    count += 1\n","    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n","    if count % 10 == 1:\n","      print(str(count)+'th iteration, loss is : ', sess.run(cross_entropy_loss,\n","                                 feed_dict={x: x_train, y_label: y_train}))\n","    if count % 100 == 0:\n","      save_path = saver.save(sess,checkpoint_path,global_step=count)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":720,"status":"ok","timestamp":1575796343532,"user":{"displayName":"Hsiank Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAiiKyodz2YML7TF4SSdbR6D3LXJFov--1YfumW=s64","userId":"05870242481039192582"},"user_tz":-480},"id":"wfb9q2I-hbJX","outputId":"54a289cb-6159-476b-9655-e60c89fef5d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files are saved\n"]}],"source":["# output\n","vectors = sess.run(W1 + b1)\n","\n","\n","def word_vec(word):\n","    v_w = vectors[word2int[word]]\n","    return v_w\n","\n","\n","def word_sim(word, top_n):\n","    w1_index = word2int[word]\n","    v_w1 = vectors[w1_index]\n","    word_sim = {}\n","    for i in range(vectors.shape[0]):\n","        v_w2 = vectors[i]\n","        theta_num = np.dot(v_w1, v_w2)\n","        theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n","        theta = theta_num / theta_den\n","        word = int2word[i]\n","        word_sim[word] = theta\n","    words_sorted = sorted(word_sim.items(), key=lambda x: x[1], reverse=True)\n","    return words_sorted[1:top_n + 1]\n","\n","\n","\n","filename_w2v = './output/w2v_matrix.txt'\n","with open(filename_w2v, 'w') as f:\n","    f.write(\"字\\t对应向量\\t训练样本数\"+str(N))\n","    for x in words:\n","        f.write(\"\\n\"+x+\"\\t\"+str(word_vec(x)))\n","filename_w2v_sample = \"/content/drive/My Drive/Colab Notebooks/w2v_sample.txt\"\n","sample = ['思', '悲', '忧', '愁', '怒', '惧', '乐']\n","with open(filename_w2v_sample, 'w') as f:\n","    for mu in sample:\n","        f.write(mu+\": \" + \"、\".join([y[0] for y in word_sim(mu, 7)])+\"\\n\")\n","\n","print('Files are saved')"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"O7JIhDfNBUS-"},"outputs":[],"source":[]}]}