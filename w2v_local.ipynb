{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w2v.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/hsianktin/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nPreparation Done\n"}],"source":["# Since I'm using colabs, we need actually implement all these into tensorflow modules so that we can use the gpu.\n","# %tensorflow_version 1.x\n","file = open('Tang_poems_utf_8.txt')\n","text = file.read()\n","file.close()\n","import matplotlib.pyplot as plt\n","## split and create regular poems\n","import re\n","import numpy as np\n","import os\n","from collections import defaultdict\n","import numpy as np\n","import tensorflow as tf\n","\n","pattern = u'卷.[0-9]+'\n","poems = re.split(pattern, text)[1:]\n","regular_poems = []\n","regular_title = []\n","corpus = []\n","N = 100 # total number of samples\n","stopwords = re.compile(u'而|何|乎|乃|其|且|然|若|所|为|焉|也|以|矣|于|之|则|者|与|欤|因|-')\n","for poem in poems:\n","    tmp_poem = poem.strip('\\n\\u3000\\u3000◎')\n","    tmp_poem = tmp_poem.replace('\\u3000\\u3000','').split('\\n')\n","    regular_title.append(tmp_poem[0])\n","    regular_poems.append('\\n'.join(tmp_poem[1:]))\n","    tmp_poem=''.join(tmp_poem[1:]).replace('\\n',\"\").replace('。','').replace('，','').replace('：','').replace('；','').replace('？','').replace('！','').replace('（[.*]*?）','')\n","    corpus.append(list(stopwords.sub('',tmp_poem)))\n","    ## Word frequency\n","    \n","#corpus=corpus\n","# corpus=corpus\n","corpus = corpus[:N]\n","# develop based on this part\n","# create a vocabulary\n","words = []\n","for poem in corpus:\n","    for word in poem:\n","        words.append(word)\n","words = set(words)\n","# create map between word and integers\n","word2int = {}\n","int2word = {}\n","vocab_size = len(words)  # gives the total number of unique words\n","for i, word in enumerate(words):\n","    word2int[word] = i\n","    int2word[i] = word\n","sentences = corpus\n","# capture each word and their neighborhood\n","data = []\n","WINDOW_SIZE = 2\n","n_count = 0\n","for sentence in sentences:\n","    for word_index, word in enumerate(sentence):\n","        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0):min(word_index + WINDOW_SIZE, len(sentence)+1)]:\n","            if nb_word != word:\n","                data.append([word, nb_word])\n","# BUT actually we don't need this, because we consider each character in the poem as a word, that is to say, a vector\n","# convert to unit vector.\n","\n","\n","def unit_vectorization(data_point_index, vocab_size):\n","    temp = np.zeros(vocab_size)\n","    temp[data_point_index] = 1\n","    return temp\n","\n","vocab_size = len(words)\n","x_train = []\n","y_train = []\n","for data_word in data:\n","    x_train.append(unit_vectorization(word2int[data_word[0]], vocab_size))\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","for data_word in data:\n","    y_train.append(unit_vectorization(word2int[data_word[1]], vocab_size))\n","y_train = np.asarray(y_train)\n","\n","# convert to nparray\n","x_train = np.asarray(x_train)\n","y_train = np.asarray(y_train)\n","# make the tensorflow model\n","x = tf.placeholder(tf.float32, [None, vocab_size],name=\"x\")\n","y_label = tf.placeholder(tf.float32, shape=(None, vocab_size),name=\"y_label\")\n","\n","EMBEDDING_DIM = 5\n","W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]),name=\"W1\")\n","# noise ### this line claims a variable of tf type which is essentially a 1x5 matrix\n","b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]),name=\"b1\")\n","hidden_representation = tf.add(tf.matmul(x, W1), b1)\n","W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]),name=\"W2\")\n","b2 = tf.Variable(tf.random_normal([vocab_size]),name=\"b2\")\n","prediction = tf.nn.softmax(tf.add(tf.matmul(hidden_representation, W2), b2))\n","sess = tf.Session()\n","init = tf.global_variables_initializer()\n","sess.run(init)  # make sure you do this!\n","# define the loss function:\n","print('Preparation Done')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Start calculating\n1th iteration, loss is :  14.2352705\n2th iteration, loss is :  13.458983\n3th iteration, loss is :  12.869039\n4th iteration, loss is :  12.39673\n5th iteration, loss is :  12.008874\n6th iteration, loss is :  11.686412\n7th iteration, loss is :  11.416661\n8th iteration, loss is :  11.190302\n9th iteration, loss is :  11.0000515\n10th iteration, loss is :  10.840038\n11th iteration, loss is :  10.70538\n12th iteration, loss is :  10.592067\n13th iteration, loss is :  10.496706\n14th iteration, loss is :  10.416428\n15th iteration, loss is :  10.348841\n16th iteration, loss is :  10.291911\n17th iteration, loss is :  10.243935\n18th iteration, loss is :  10.203472\n19th iteration, loss is :  10.169301\n20th iteration, loss is :  10.140401\n21th iteration, loss is :  10.115904\n22th iteration, loss is :  10.095107\n23th iteration, loss is :  10.077386\n24th iteration, loss is :  10.062241\n25th iteration, loss is :  10.049247\n26th iteration, loss is :  10.038052\n27th iteration, loss is :  10.028352\n28th iteration, loss is :  10.019906\n29th iteration, loss is :  10.012508\n30th iteration, loss is :  10.005975\n31th iteration, loss is :  10.000175\n32th iteration, loss is :  9.994987\n33th iteration, loss is :  9.990304\n34th iteration, loss is :  9.986046\n35th iteration, loss is :  9.982149\n36th iteration, loss is :  9.978553\n37th iteration, loss is :  9.975201\n38th iteration, loss is :  9.972062\n39th iteration, loss is :  9.969101\n40th iteration, loss is :  9.966294\n41th iteration, loss is :  9.963608\n42th iteration, loss is :  9.96103\n43th iteration, loss is :  9.95854\n44th iteration, loss is :  9.956123\n45th iteration, loss is :  9.953772\n46th iteration, loss is :  9.951484\n47th iteration, loss is :  9.949238\n48th iteration, loss is :  9.947033\n49th iteration, loss is :  9.944866\n50th iteration, loss is :  9.942733\n51th iteration, loss is :  9.940621\n52th iteration, loss is :  9.938537\n53th iteration, loss is :  9.936473\n54th iteration, loss is :  9.934431\n55th iteration, loss is :  9.9324\n56th iteration, loss is :  9.930388\n57th iteration, loss is :  9.928388\n58th iteration, loss is :  9.926396\n59th iteration, loss is :  9.924422\n60th iteration, loss is :  9.9224615\n61th iteration, loss is :  9.920498\n62th iteration, loss is :  9.918554\n63th iteration, loss is :  9.916617\n64th iteration, loss is :  9.914685\n65th iteration, loss is :  9.912768\n66th iteration, loss is :  9.910857\n67th iteration, loss is :  9.90895\n68th iteration, loss is :  9.907049\n69th iteration, loss is :  9.905159\n70th iteration, loss is :  9.903275\n71th iteration, loss is :  9.901395\n72th iteration, loss is :  9.899526\n73th iteration, loss is :  9.8976555\n74th iteration, loss is :  9.895794\n75th iteration, loss is :  9.89394\n76th iteration, loss is :  9.892097\n77th iteration, loss is :  9.890249\n78th iteration, loss is :  9.888416\n79th iteration, loss is :  9.886586\n80th iteration, loss is :  9.884762\n81th iteration, loss is :  9.882941\n82th iteration, loss is :  9.881128\n83th iteration, loss is :  9.879317\n84th iteration, loss is :  9.877515\n85th iteration, loss is :  9.875719\n86th iteration, loss is :  9.873927\n87th iteration, loss is :  9.872141\n88th iteration, loss is :  9.8703575\n89th iteration, loss is :  9.868582\n90th iteration, loss is :  9.866811\n91th iteration, loss is :  9.8650465\n92th iteration, loss is :  9.863283\n93th iteration, loss is :  9.861526\n94th iteration, loss is :  9.8597765\n95th iteration, loss is :  9.858029\n96th iteration, loss is :  9.856289\n97th iteration, loss is :  9.854549\n98th iteration, loss is :  9.85282\n99th iteration, loss is :  9.851088\n100th iteration, loss is :  9.849369\n101th iteration, loss is :  9.8476515\n102th iteration, loss is :  9.845938\n103th iteration, loss is :  9.844232\n104th iteration, loss is :  9.842525\n105th iteration, loss is :  9.840827\n106th iteration, loss is :  9.83913\n107th iteration, loss is :  9.837442\n108th iteration, loss is :  9.835754\n109th iteration, loss is :  9.834075\n110th iteration, loss is :  9.832398\n111th iteration, loss is :  9.830726\n112th iteration, loss is :  9.82906\n113th iteration, loss is :  9.827395\n114th iteration, loss is :  9.825739\n115th iteration, loss is :  9.824082\n116th iteration, loss is :  9.822427\n117th iteration, loss is :  9.82078\n118th iteration, loss is :  9.819142\n119th iteration, loss is :  9.817509\n120th iteration, loss is :  9.815867\n121th iteration, loss is :  9.814241\n122th iteration, loss is :  9.8126135\n123th iteration, loss is :  9.8109865\n124th iteration, loss is :  9.809375\n125th iteration, loss is :  9.807758\n126th iteration, loss is :  9.806152\n127th iteration, loss is :  9.804544\n128th iteration, loss is :  9.8029375\n129th iteration, loss is :  9.801345\n130th iteration, loss is :  9.799747\n131th iteration, loss is :  9.798159\n132th iteration, loss is :  9.796571\n133th iteration, loss is :  9.794987\n134th iteration, loss is :  9.793411\n135th iteration, loss is :  9.791837\n136th iteration, loss is :  9.790268\n137th iteration, loss is :  9.788697\n138th iteration, loss is :  9.787136\n139th iteration, loss is :  9.785575\n140th iteration, loss is :  9.784018\n141th iteration, loss is :  9.782465\n142th iteration, loss is :  9.780913\n143th iteration, loss is :  9.779371\n144th iteration, loss is :  9.777834\n145th iteration, loss is :  9.776286\n146th iteration, loss is :  9.77475\n147th iteration, loss is :  9.773218\n148th iteration, loss is :  9.7716875\n149th iteration, loss is :  9.77017\n150th iteration, loss is :  9.7686405\n151th iteration, loss is :  9.76713\n152th iteration, loss is :  9.765614\n153th iteration, loss is :  9.764103\n154th iteration, loss is :  9.762594\n155th iteration, loss is :  9.76109\n156th iteration, loss is :  9.759589\n157th iteration, loss is :  9.758095\n158th iteration, loss is :  9.756603\n159th iteration, loss is :  9.755103\n160th iteration, loss is :  9.753621\n161th iteration, loss is :  9.752132\n162th iteration, loss is :  9.750649\n163th iteration, loss is :  9.749175\n164th iteration, loss is :  9.747696\n165th iteration, loss is :  9.746225\n166th iteration, loss is :  9.744751\n167th iteration, loss is :  9.743289\n168th iteration, loss is :  9.741826\n169th iteration, loss is :  9.740368\n170th iteration, loss is :  9.738907\n171th iteration, loss is :  9.737457\n172th iteration, loss is :  9.736005\n173th iteration, loss is :  9.734553\n174th iteration, loss is :  9.733108\n175th iteration, loss is :  9.731669\n176th iteration, loss is :  9.730227\n177th iteration, loss is :  9.728795\n178th iteration, loss is :  9.727361\n179th iteration, loss is :  9.725933\n180th iteration, loss is :  9.7245035\n181th iteration, loss is :  9.723078\n182th iteration, loss is :  9.721658\n183th iteration, loss is :  9.720237\n184th iteration, loss is :  9.718823\n185th iteration, loss is :  9.717407\n186th iteration, loss is :  9.715997\n187th iteration, loss is :  9.714587\n188th iteration, loss is :  9.713183\n189th iteration, loss is :  9.711786\n190th iteration, loss is :  9.710381\n191th iteration, loss is :  9.708984\n192th iteration, loss is :  9.707593\n193th iteration, loss is :  9.706202\n194th iteration, loss is :  9.704814\n195th iteration, loss is :  9.703425\n196th iteration, loss is :  9.702048\n197th iteration, loss is :  9.700662\n198th iteration, loss is :  9.699285\n199th iteration, loss is :  9.69791\n200th iteration, loss is :  9.696542\n201th iteration, loss is :  9.695168\n202th iteration, loss is :  9.693796\n203th iteration, loss is :  9.692435\n204th iteration, loss is :  9.691076\n205th iteration, loss is :  9.689709\n206th iteration, loss is :  9.688351\n207th iteration, loss is :  9.6869955\n208th iteration, loss is :  9.68565\n209th iteration, loss is :  9.684295\n210th iteration, loss is :  9.682948\n211th iteration, loss is :  9.681601\n212th iteration, loss is :  9.68026\n213th iteration, loss is :  9.678916\n214th iteration, loss is :  9.677579\n215th iteration, loss is :  9.67624\n216th iteration, loss is :  9.674911\n217th iteration, loss is :  9.673578\n218th iteration, loss is :  9.672257\n219th iteration, loss is :  9.670926\n220th iteration, loss is :  9.669598\n221th iteration, loss is :  9.6682825\n222th iteration, loss is :  9.666959\n223th iteration, loss is :  9.665644\n224th iteration, loss is :  9.6643305\n225th iteration, loss is :  9.663017\n226th iteration, loss is :  9.661711\n227th iteration, loss is :  9.6604\n228th iteration, loss is :  9.659093\n229th iteration, loss is :  9.657791\n230th iteration, loss is :  9.656487\n231th iteration, loss is :  9.655194\n232th iteration, loss is :  9.653893\n233th iteration, loss is :  9.652601\n234th iteration, loss is :  9.651308\n235th iteration, loss is :  9.650018\n236th iteration, loss is :  9.64873\n237th iteration, loss is :  9.647444\n238th iteration, loss is :  9.646157\n239th iteration, loss is :  9.644883\n240th iteration, loss is :  9.6436\n241th iteration, loss is :  9.642324\n242th iteration, loss is :  9.64105\n243th iteration, loss is :  9.639773\n244th iteration, loss is :  9.638505\n245th iteration, loss is :  9.637235\n246th iteration, loss is :  9.635966\n247th iteration, loss is :  9.634704\n248th iteration, loss is :  9.633437\n249th iteration, loss is :  9.632184\n250th iteration, loss is :  9.630925\n251th iteration, loss is :  9.629669\n252th iteration, loss is :  9.628409\n253th iteration, loss is :  9.627157\n254th iteration, loss is :  9.62591\n255th iteration, loss is :  9.6246605\n256th iteration, loss is :  9.623417\n257th iteration, loss is :  9.622171\n258th iteration, loss is :  9.620925\n259th iteration, loss is :  9.619681\n260th iteration, loss is :  9.618448\n261th iteration, loss is :  9.617214\n262th iteration, loss is :  9.615975\n263th iteration, loss is :  9.614744\n264th iteration, loss is :  9.613511\n265th iteration, loss is :  9.612282\n266th iteration, loss is :  9.6110525\n267th iteration, loss is :  9.609829\n268th iteration, loss is :  9.608604\n269th iteration, loss is :  9.607384\n270th iteration, loss is :  9.606164\n271th iteration, loss is :  9.604946\n272th iteration, loss is :  9.603736\n273th iteration, loss is :  9.60252\n274th iteration, loss is :  9.601299\n275th iteration, loss is :  9.600088\n276th iteration, loss is :  9.598888\n277th iteration, loss is :  9.597677\n278th iteration, loss is :  9.596475\n279th iteration, loss is :  9.595271\n280th iteration, loss is :  9.5940695\n281th iteration, loss is :  9.592866\n282th iteration, loss is :  9.591669\n283th iteration, loss is :  9.590471\n284th iteration, loss is :  9.589278\n285th iteration, loss is :  9.588096\n286th iteration, loss is :  9.586894\n287th iteration, loss is :  9.585708\n288th iteration, loss is :  9.5845175\n289th iteration, loss is :  9.583332\n290th iteration, loss is :  9.5821495\n291th iteration, loss is :  9.580962\n292th iteration, loss is :  9.579788\n293th iteration, loss is :  9.578605\n294th iteration, loss is :  9.577426\n295th iteration, loss is :  9.576254\n296th iteration, loss is :  9.575078\n297th iteration, loss is :  9.57391\n298th iteration, loss is :  9.572739\n299th iteration, loss is :  9.571567\n300th iteration, loss is :  9.570401\n301th iteration, loss is :  9.56923\n"}],"source":["# define the loss function:\n","cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(\n","    y_label * tf.log(prediction), reduction_indices=[1]))\n","# define the training step:\n","train_step = tf.train.GradientDescentOptimizer(\n","    0.1).minimize(cross_entropy_loss)\n","n_iters = 301 # Even on google colab implementing 10000 iterations takes a very looooong time\n","print('Start calculating')\n","# train for n_iter iterations\n","saver = tf.train.Saver()\n","checkpoint_path = \"./ckpt/w2v\" \n","count = 0\n","for _ in range(n_iters):\n","    count += 1\n","    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n","    print(str(count)+'th iteration, loss is : ', sess.run(cross_entropy_loss,\n","                                 feed_dict={x: x_train, y_label: y_train}))\n","    if count % 100 == 0:\n","      save_path = saver.save(sess,checkpoint_path,global_step=count)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'怒'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c8fdef75c5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_w2v_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"、\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files are saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-c8fdef75c5af>\u001b[0m in \u001b[0;36mword_sim\u001b[0;34m(word, top_n)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mw1_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mv_w1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mword_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: '怒'"]}],"source":["# output\n","vectors = sess.run(W1 + b1)\n","\n","\n","def word_vec(word):\n","    v_w = vectors[word2int[word]]\n","    return v_w\n","\n","\n","def word_sim(word, top_n):\n","    w1_index = word2int[word]\n","    v_w1 = vectors[w1_index]\n","    word_sim = {}\n","    for i in range(vectors.shape[0]):\n","        v_w2 = vectors[i]\n","        theta_num = np.dot(v_w1, v_w2)\n","        theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n","        theta = theta_num / theta_den\n","        word = int2word[i]\n","        word_sim[word] = theta\n","    words_sorted = sorted(word_sim.items(), key=lambda x: x[1], reverse=True)\n","    return words_sorted[1:top_n + 1]\n","\n","\n","\n","filename_w2v = './output/w2v_matrix_100.txt'\n","with open(filename_w2v, 'w') as f:\n","    f.write(\"字\\t对应向量\\t训练样本数\"+str(N))\n","    for x in words:\n","        f.write(\"\\n\"+x+\"\\t\"+str(word_vec(x)))\n","filename_w2v_sample = \"./output/w2v_matrix_100_sample.txt\"\n","sample = ['思', '悲', '忧', '愁', '怒', '惧', '乐']\n","with open(filename_w2v_sample, 'w') as f:\n","    for mu in sample:\n","        f.write(mu+\": \" + \"、\".join([y[0] for y in word_sim(mu, 7)])+\"\\n\")\n","\n","print('Files are saved')"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"O7JIhDfNBUS-"},"outputs":[],"source":[]}]}